# ELM_MinID
> Extreme learning machine under minimum information divergence criterion

|Author|Chengtian Song,Lizhi Pan|
|---|---
|Version|1.0.0|
|Platform|MATLB 2018b|

****

  In recent years, extreme learning machine (ELM) and its improved algorithms have been successfully applied to various classication and regression tasks. In
these algorithms, MSE criterion is commonly used to control training error. However, MSE criterion is not suitable to deal with outliers, which can exist in
general regression or classication tasks. In this paper, a novel extreme learning machine under minimum information divergence criterion (ELM-MinID) is
proposed to deal with the training set with noises. In minimum information divergence criterion, the Gaussian kernel function and Euclidean information
divergence are utilized to substitute the mean square error (MSE) criterion to enhance the anti-noise ability of ELM. Experimental results on two synthetic
datasets and nine benchmark datasets show that this method is superior to traditional ELMs.

![](header.png)



## Usage example

![image](https://github.com/Panlizhi/ELM_MinID/blob/master/result_picture.png)
![image](https://github.com/Panlizhi/ELM_MinID/blob/master/Snipaste_2020-06-05_17-51-31.png)
![image](https://github.com/Panlizhi/ELM_MinID/blob/master/Snipaste_2020-06-05_17-50-18.png)
![image](https://github.com/Panlizhi/ELM_MinID/blob/master/Snipaste_2020-06-05_17-50-48.png)
![image](https://github.com/Panlizhi/ELM_MinID/blob/master/Snipaste_2020-06-05_17-51-02.png)
## Development setup

Please run the code in matlabã€‚
```sh
ELM_MinID.m
```

## Release History

* 1.0.0
    * Work is been created

